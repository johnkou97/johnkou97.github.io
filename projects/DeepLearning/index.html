<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="y6DuRnn9lWE2Ft2UEL3CzC8k_I5e3nm1wRPCuK2akDM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deep Learning | Ioannis Koutalios </title> <meta name="author" content="Ioannis Koutalios"> <meta name="description" content="projects from the course 'Deep Learning' at Leiden University"> <meta name="keywords" content="astrophysics, data-science, AI, machine-learning, astronomy"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?7cb15ee97cbbaab8d2e46ad89b3ffac3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://koutalios.space/projects/DeepLearning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ioannis</span> Koutalios </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep Learning</h1> <p class="post-description">projects from the course 'Deep Learning' at Leiden University</p> </header> <article> <p>Cover Image by <a href="https://unsplash.com/@growtika?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="external nofollow noopener" target="_blank">Growtika</a> on <a href="https://unsplash.com/photos/an-abstract-image-of-a-sphere-with-dots-and-lines-nGoCBxiaRO0?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="external nofollow noopener" target="_blank">Unsplash</a> <br> <br></p> <p>The code for all the projects can be found in my <a href="https://github.com/johnkou97/DeepLearning" rel="external nofollow noopener" target="_blank">GitHub repository</a>. The two reports for the assignments can be found <a href="/assets/pdf/DeepLearning_1.pdf">here</a> and <a href="/assets/pdf/DeepLearning_2.pdf">here</a>.</p> <h2 id="data-dimensionality---distance-based-classification">Data Dimensionality - Distance Based Classification</h2> <p>In this assignment, we explored the MNIST dataset and applied dimensionality reduction techniques to visualize the data. The dataset consists of 2707 images (1707 for training and 1000 for testing) of handwritten digits from 0 to 9. The images are 16x16 pixels in size and are represented as a vector of 256 features. Each sample is labeled with the corresponding digit.</p> <p>The first step was to calculate the average digit for each class in the dataset. The average digits are shown in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/av_digits.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/av_digits.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="av_digits" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Average digits from the MNIST dataset. </div> <p>We can alo calculate the centre for each digit class and use it as a reference point for the distance-based classification. One measure of how easy is to separate two different digits is the distance between their centres. The distances can be seen in the table below.</p> <table> <thead> <tr> <th> </th> <th>0</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> <th>9</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>0.0</td> <td>14.4</td> <td>9.3</td> <td>9.1</td> <td>10.8</td> <td>7.5</td> <td>8.2</td> <td>11.9</td> <td>9.9</td> <td>11.5</td> </tr> <tr> <td>1</td> <td>14.4</td> <td>0.0</td> <td>10.1</td> <td>11.7</td> <td>10.2</td> <td>11.1</td> <td>10.6</td> <td>10.7</td> <td>10.1</td> <td>9.9</td> </tr> <tr> <td>2</td> <td>9.3</td> <td>10.1</td> <td>0.0</td> <td>8.2</td> <td>7.9</td> <td>7.9</td> <td>7.3</td> <td>8.9</td> <td>7.1</td> <td>8.9</td> </tr> <tr> <td>3</td> <td>9.1</td> <td>11.7</td> <td>8.2</td> <td>0.0</td> <td>9.1</td> <td>6.1</td> <td>9.3</td> <td>8.9</td> <td>7.0</td> <td>8.4</td> </tr> <tr> <td>4</td> <td>10.8</td> <td>10.2</td> <td>7.9</td> <td>9.1</td> <td>0.0</td> <td>8.0</td> <td>8.8</td> <td>7.6</td> <td>7.4</td> <td>6.0</td> </tr> <tr> <td>5</td> <td>7.5</td> <td>11.1</td> <td>7.9</td> <td>6.1</td> <td>8.0</td> <td>0.0</td> <td>6.7</td> <td>9.2</td> <td>7.0</td> <td>8.3</td> </tr> <tr> <td>6</td> <td>8.2</td> <td>10.6</td> <td>7.3</td> <td>9.3</td> <td>8.8</td> <td>6.7</td> <td>0.0</td> <td>10.9</td> <td>8.6</td> <td>10.4</td> </tr> <tr> <td>7</td> <td>11.9</td> <td>10.7</td> <td>8.9</td> <td>8.9</td> <td>7.6</td> <td>9.2</td> <td>10.9</td> <td>0.0</td> <td>8.5</td> <td>5.4</td> </tr> <tr> <td>8</td> <td>9.9</td> <td>10.1</td> <td>7.1</td> <td>7.0</td> <td>7.4</td> <td>7.0</td> <td>8.6</td> <td>8.5</td> <td>0.0</td> <td>6.4</td> </tr> <tr> <td>9</td> <td>11.5</td> <td>9.9</td> <td>8.9</td> <td>8.4</td> <td>6.0</td> <td>8.3</td> <td>10.4</td> <td>5.4</td> <td>6.4</td> <td>0.0</td> </tr> </tbody> </table> <p></p> <p>We can see that the distance between the digits is not uniform, which means that some digits are easier to separate than others. The pair of digits with the smallest distance is 7 and 9, which means that those two digits are the hardest to separate in a distance-based classification.</p> <p>The next step was to apply dimensionality reduction techniques to visualize the data. We used Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP) to reduce the dimensionality of the data to 2D. The results are shown in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/pca.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/pca.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/tsne.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/tsne.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="tsne" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/umap.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/umap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="umap" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Dimensionality reduction techniques for the MNIST dataset. From left to right: PCA, t-SNE, and UMAP. </div> <p>We can see that the different digits are separated in the 2D space, which means that the dimensionality reduction techniques were able to capture the underlying structure of the data. As we can see, the distances we calculated earlier are reflected in the 2D space, with some digits being closer to each other than others.</p> <p>We applied two classification algorithms to the reduced data: a simple distance-based classifier, where we assigned the label of the closest centre, and a k-Nearest Neighbors (k-NN) classifier, where we assigned the label based on the majority vote of the k closest neighbours. We only tested \(k=3\) for the k-NN classifier. The results are shown in the table below.</p> <table> <thead> <tr> <th>Classifier</th> <th>Dataset</th> <th>Correct</th> <th>Incorrect</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Distance</td> <td>Train</td> <td>1474</td> <td>233</td> <td>86.35%</td> </tr> <tr> <td> </td> <td>Test</td> <td>804</td> <td>196</td> <td>80.40%</td> </tr> <tr> <td>k-NN</td> <td>Train</td> <td>1671</td> <td>36</td> <td>97.89%</td> </tr> <tr> <td> </td> <td>Test</td> <td>914</td> <td>86</td> <td>91.40%</td> </tr> </tbody> </table> <p></p> <p>We can see that the k-NN classifier outperformed the distance-based classifier on both the training and test datasets.</p> <h2 id="multiclass-perceptron">MultiClass Perceptron</h2> <p>For this assignment, we implemented a multi-class perceptron model to classify the MNIST dataset. The implementation was done from scratch in Python using only the <code class="language-plaintext highlighter-rouge">numpy</code> library (no deep learning frameworks were used).</p> <p>We used two different techniques to train the model: giving the model all the training data at once (batch training) and giving the model one sample at a time (online training). The results are shown in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/accuracy.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/accuracy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="accuracy" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Accuracy of the multi-class perceptron model on the MNIST dataset for batch (Whole Dataset Method) and online (Input one by one Method) training. </div> <p>Training the model with the whole dataset at once required more epochs to converge compared to training the model one sample at a time. However, the training time was significantly shorter for the batch training method. The accuracies of the two methods can be seen in the table below.</p> <table> <thead> <tr> <th>Method</th> <th>Dataset</th> <th>Correct</th> <th>Incorrect</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Batch</td> <td>Train</td> <td>1683</td> <td>24</td> <td>98.59%</td> </tr> <tr> <td> </td> <td>Test</td> <td>881</td> <td>119</td> <td>88.10%</td> </tr> <tr> <td>Online</td> <td>Train</td> <td>1707</td> <td>0</td> <td>100.00%</td> </tr> <tr> <td> </td> <td>Test</td> <td>872</td> <td>128</td> <td>87.20%</td> </tr> </tbody> </table> <p></p> <p>We can see that the online training method achieved a higher accuracy on the training dataset compared to the batch training method. However, the test accuracy was slightly lower for the online training method. This can be caused by overfitting to the training data when training the model one sample at a time. On the test dataset, both methods achieved similar accuracies, with the batch training method slightly outperforming the online training method.</p> <h2 id="xor-network---gradient-descent">XOR Network - Gradient Descent</h2> <p>We implemented a simple neural network to solve the XOR problem using gradient descent. The XOR problem is a classic example of a non-linearly separable problem that cannot be solved by a single perceptron. The XOR takes as input two binary values (0 or 1) and outputs 1 if the inputs are different and 0 if the inputs are the same.</p> <p>In our implementation, we used a neural network with one hidden layer and one output layer. The hidden layer has three neurons, and the output layer has one neuron. We trained the network using gradient descent and the backpropagation algorithm. We tested three different learning rates: 0.3, 0.6, and 0.8. The results are shown in the figure below. We also tested the network with two different initializations: uniform and normal. The mean squared error (MSE) was used as the loss function. We can see the training process for the different learning rates and initializations in the figures below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/mse_1.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/mse_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mse_1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/mse_2.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/mse_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mse_2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Mean squared error (MSE) during training for different learning rates and initializations. Left: varying learning rates with uniform initialization. Right: varying initializations with learning rate 0.3. </div> <p>We can see that all the different strategies converged to a solution, reaching an MSE of 0, which means that the network was able to solve the XOR problem. From the diffefrent learning rates, we can see that the highest learning rate (0.8) converged faster than the other two. The uniform initialization also converged faster than the normal initialization.</p> <p>We also implemented a lazy approach to the XOR problem, where we used the same network architecture but in each epoch, we randomized the weights of the network, until the network was able to solve the XOR problem. The test was to see how many epochs, on average, it took for the network to solve the XOR problem. We did four attempts with an average of 118570.75 epochs and a standard deviation of 118409.22 epochs. The results show that the network was able to solve the XOR problem, but it took a large number of epochs to do so, making it an inefficient approach.</p> <h2 id="fashion-mnist--cifar-10---tensorflow">Fashion MNIST &amp; CIFAR 10 - TensorFlow</h2> <p>In this assignment, we used TensorFlow to build and train neural networks on the Fashion MNIST and CIFAR-10 datasets. The Fashion MNIST dataset consists of 70,000 grayscale images (60,000 for training and 10,000 for testing) of 28x28 pixels each, representing 10 different classes of clothing items. The CIFAR-10 dataset consists of 60,000 color images (50,000 for training and 10,000 for testing) of 32x32x3 pixels each, representing 10 different classes of objects.</p> <p>We built two different neural network architectures. The first architecture is a multi-layer perceptron (MLP) with three hidden layers, each with 300, 100, and 10 neurons, respectively. The total number of parameters in the model is 266,610. The second architecture is a convolutional neural network (CNN) with three convolutional layers, each followed by a max-pooling layer, and two fully connected layers. The total number of parameters in the model is 1,413,834. We trained both models on the Fashion MNIST and CIFAR-10 datasets each time only changing the input layer to match the dataset. For each dataset we selected 5,000 samples from the training set to use as the validation set. The results are shown in the table below.</p> <table> <thead> <tr> <th>Dataset</th> <th>Model</th> <th>Train Accuracy</th> <th>Validation Accuracy</th> <th>Test Accuracy</th> </tr> </thead> <tbody> <tr> <td>Fashion MNIST</td> <td>MLP</td> <td>0.92</td> <td>0.88</td> <td>0.85</td> </tr> <tr> <td> </td> <td>CNN</td> <td>0.92</td> <td>0.89</td> <td>0.22</td> </tr> <tr> <td>CIFAR-10</td> <td>MLP</td> <td>0.63</td> <td>0.48</td> <td>0.38</td> </tr> <tr> <td> </td> <td>CNN</td> <td>0.75</td> <td>0.71</td> <td>0.54</td> </tr> </tbody> </table> <p></p> <p>For the Fashion MNIST dataset, the MLP model achieved a test accuracy of 85%, while the CNN model had a very low test accuracy of 22%, while for the training and validation accuracy both models performed similarly. This indicates that the CNN model was overfitting the data. For the CIFAR-10 dataset, the MLP model achieved a test accuracy of 38%, while the CNN model had a test accuracy of 54%. The CNN model outperformed the MLP model on the CIFAR-10 dataset, indicating that the CNN model was able to capture the spatial features of the images better than the MLP model, which is something that was expected, since CNNs are more suitable for image classification tasks.</p> <h2 id="tell-the-time-network">“Tell the Time” Network</h2> <p>The goal of this assignment is to develop a model that can tell the time from an image of an analog clock. The dataset consists of 18,000 images of analog clocks, each labeled with two values: the hour and the minute. The images are grayscale and have a resolution of 150x150 pixels. Each image is taken from a different angle and rotation, making the task far more challenging. The full dataset can be found <a href="https://surfdrive.surf.nl/files/index.php/s/B8emtQRGUeAaqmz" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>We used three different approaches to solve the problem:</p> <ul> <li>Regression: where we trained a neural network to predict a single output value for the hour and minute (e.g., 12:30 -&gt; 12.5).</li> <li>Classification: where we trained a neural network to classify the hour and minute into discrete classes (we varied the number of classes from 24 to 60).</li> <li>Multi-head: where we trained a neural network with two output heads, one for the hour and one for the minute. For the hour we used classification with 12 classes (one for each hour), and for the minute we used regression.</li> </ul> <p>For the accuracy measure, we used a “common sense” accuracy, which is the absolute value of the time difference between the predicted and the actual time. For example, the “common sense” difference between a “predicted” time of 11:50 and the “target” time of 0:10 is just 20 minutes and not 11 hours and 40 minutes. Minimizing this “common sense” error measure was the main objective of this assignment.</p> <p>For both approaches (classification and regression), we used a neural network with three convolutional layers, each followed by a max-pooling layer, and two fully connected layers. For the regression we tested using both the mean squared error (MSE) and the “common sense” accuracy as the loss function. The results are shown in the table below.</p> <table> <thead> <tr> <th>Approach</th> <th>Loss Function / Number of Classes</th> <th>Train Accuracy (minutes)</th> <th>Test Accuracy (minutes)</th> </tr> </thead> <tbody> <tr> <td>Regression</td> <td>MSE</td> <td>13.44</td> <td>47.66</td> </tr> <tr> <td> </td> <td>“Common Sense”</td> <td>16.93</td> <td>33.30</td> </tr> <tr> <td>Classification</td> <td>24 classes</td> <td>14.62</td> <td>24.79</td> </tr> <tr> <td> </td> <td>48 classes</td> <td>7.09</td> <td>22.89</td> </tr> <tr> <td> </td> <td>120 classes</td> <td>3.51</td> <td>35.81</td> </tr> <tr> <td> </td> <td>240 classes</td> <td>1.98</td> <td>41.15</td> </tr> <tr> <td> </td> <td>480 classes</td> <td>1.71</td> <td>56.34</td> </tr> <tr> <td> </td> <td>720 classes</td> <td>1.53</td> <td>83.38</td> </tr> <tr> <td>Multi-head</td> <td>12 classes (hour) + MSE (minute)</td> <td>4.91</td> <td>11.71</td> </tr> </tbody> </table> <p></p> <p>The “common sense” accuracy improved the test accuracy of the regression approach, indicating that it is a better loss function for this task. The classification approach outperformed the regression approach, with the best classification model (48 classes) achieving a test accuracy of 22.89 minutes. The multi-head approach outperformed both the regression and classification approaches, with a test accuracy of 11.71 minutes. This indicates that the multi-head approach was able to capture the hour and minute information separately, which improved the overall accuracy of the model. It is also interesting to note how increasing the number of classes after a certain point (48 classes) did not improve the accuracy on the test set, while it kept improving the accuracy on the training set. This indicates that the model was overfitting the training data when the number of classes was too high. This is also illustrated nicely in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/classification.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/classification.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="classification" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Accuracy of the classification approach on the training and test sets for different numbers of classes. </div> <p>Overall, the assignment was a success, and we were able to develop a model that can tell the time from an image of an analog clock (with many different angles and rotations), with a “common sense” accuracy of 11.71 minutes on unseen data.</p> <h2 id="generative-models---autoencoders-vaes--generative-adversarial-networks-gans">Generative Models - Autoencoders (VAEs) &amp; Generative Adversarial Networks (GANs)</h2> <p>In this assignment, we explored generative models, specifically autoencoders (Variational Autoencoders - VAEs) and Generative Adversarial Networks (GANs). We started with a <a href="https://colab.research.google.com/drive/17Rk3MLC6XAxaTQ--M9L_LwM_K1OId-hj?usp=sharing" rel="external nofollow noopener" target="_blank">jupyter notebook</a> that introduced the concepts of autoencoders and GANs and provided a step-by-step guide on how to implement them using TensorFlow and Keras. The notebook used a <a href="https://surfdrive.surf.nl/files/index.php/s/6TTwiWuIRr6wppl" rel="external nofollow noopener" target="_blank">dataset of faces</a> to train the models and generate new faces. In our approach, we used the <a href="https://www.kaggle.com/datasets/kostastokis/simpsons-faces" rel="external nofollow noopener" target="_blank">Simpsons Faces dataset</a> to train the models and generate new faces. The original images from the faces dataset and the Simpsons faces dataset are shown in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/original.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="original" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Original images from the faces dataset (left) and the Simpsons faces dataset (right). </div> <p>The VAE and GAN models were trained on the faces dataset and the Simpsons faces dataset. The results of the training process are shown in the figures below. We can see that the VAE model was able to generate new faces that resemble the original faces from the first epoch to the last epoch. The GAN model was also able to generate new faces, but only after a few epochs. Although the results are not perfect, we can see that the models were able to capture the underlying structure of the data and generate new faces that resemble the original faces for both datasets.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/original_vae_0.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/original_vae_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="original_vae_0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/original_vae_1.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/original_vae_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="original_vae_1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> VAE generated images from the faces dataset at the beginning of the training process (left) and at the end of the training process (right). </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/original_gan_0.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/original_gan_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="original_gan_0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/original_gan_1.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/original_gan_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="original_gan_1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> GAN generated images from the faces dataset at the beginning of the training process (left) and at the end of the training process (right). </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps_vae_0.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps_vae_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps_vae_0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps_vae_1.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps_vae_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps_vae_1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> VAE generated images from the Simpsons faces dataset at the beginning of the training process (left) and at the end of the training process (right). </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps_gan_0.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps_gan_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps_gan_0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps_gan_1.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps_gan_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps_gan_1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> GAN generated images from the Simpsons faces dataset at the beginning of the training process (left) and at the end of the training process (right). </div> <p>As a last step, we generated new images by interpolating between two random latent vectors in the latent space of the VAE and GAN models. The results are shown in the figures below. The results for the GAN model were not great, but we can see that the VAE model was somewhat able to generate new faces by interpolating between two random latent vectors.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps_vae_inter.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps_vae_inter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps_vae_inter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/deep/simps_gan_inter.png" sizes="95vw"></source> <img src="/assets/img/courses/deep/simps_gan_inter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simps_gan_inter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> VAE (left) and GAN (right) generated images from the Simpsons faces dataset at the end of the training process with interpolation between two random latent vectors. </div> <p>Overall, the assignment was a great introduction to generative models. Although the results were not perfect, the approach that was used was very simple and only scratched the surface of what is possible with generative models.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ioannis Koutalios. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: November 25, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"Publications in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Some of the projects I have worked on. The projects are categorized into research, course and personal projects. You can find more information about each project by clicking on the cards below.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-data-mining",title:"Data Mining",description:"projects from the course 'Data Mining' at Leiden University",section:"Projects",handler:()=>{window.location.href="/projects/DataMining/"}},{id:"projects-deep-learning",title:"Deep Learning",description:"projects from the course 'Deep Learning' at Leiden University",section:"Projects",handler:()=>{window.location.href="/projects/DeepLearning/"}},{id:"projects-applying-a-cross-power-model-for-power-suppression-in-flamingo",title:"Applying a cross-power model for power suppression in FLAMINGO",description:"Master's thesis research project (2nd year)",section:"Projects",handler:()=>{window.location.href="/projects/FLAMINGO/"}},{id:"projects-gravitational-wave-hackathon",title:"Gravitational Wave Hackathon",description:"g2net hackathon in Malta (2020) and Thessaloniki (2023)",section:"Projects",handler:()=>{window.location.href="/projects/GW_hackathon/"}},{id:"projects-model-free-reinforcement-learning-for-sensorless-adaptive-optics",title:"Model-Free Reinforcement Learning for Sensorless Adaptive Optics",description:"research project on Advanced Reinforcement Learning",section:"Projects",handler:()=>{window.location.href="/projects/RLAdaptive/"}},{id:"projects-other-astronomical-projects",title:"Other Astronomical Projects",description:"projects from various astronomical, astrophysical, and mathematical courses",section:"Projects",handler:()=>{window.location.href="/projects/astronomical/"}},{id:"projects-finding-the-best-exoplanets-to-search-for-exomoons-by-radial-velocity",title:"Finding the best exoplanets to search for exomoons by radial velocity",description:"Master's thesis research project (1st year)",section:"Projects",handler:()=>{window.location.href="/projects/exomoons/"}},{id:"projects-kaggle-challenges",title:"Kaggle Challenges",description:"collection of Kaggle challenges I have participated in.",section:"Projects",handler:()=>{window.location.href="/projects/kaggle/"}},{id:"projects-reinforcement-learning",title:"Reinforcement Learning",description:"projects from the course 'Reinforcement Learning' at Leiden University",section:"Projects",handler:()=>{window.location.href="/projects/reinforcement/"}},{id:"projects-spectral-classification-of-neutron-star-post-merger-gravitational-wave-emission",title:"Spectral Classification of Neutron Star Post-Merger Gravitational Wave Emission",description:"Bachelor's thesis research project",section:"Projects",handler:()=>{window.location.href="/projects/spectral/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%6B%6F%75%74%61%6C%69%6F%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/306989855203","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0004-5933-5371","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/johnkou97","_blank")}},{id:"socials-gitlab",title:"GitLab",section:"Socials",handler:()=>{window.open("https://gitlab.com/johnkou97","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/ioanniskoutalios","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>