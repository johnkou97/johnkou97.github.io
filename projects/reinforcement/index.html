<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="y6DuRnn9lWE2Ft2UEL3CzC8k_I5e3nm1wRPCuK2akDM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning | Ioannis Koutalios </title> <meta name="author" content="Ioannis Koutalios"> <meta name="description" content="projects from the course 'Reinforcement Learning' at Leiden University"> <meta name="keywords" content="astrophysics, data-science, AI, machine-learning, astronomy"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?7cb15ee97cbbaab8d2e46ad89b3ffac3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://koutalios.space/projects/reinforcement/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ioannis</span> Koutalios </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning</h1> <p class="post-description">projects from the course 'Reinforcement Learning' at Leiden University</p> </header> <article> <p>Cover Image by <a href="https://unsplash.com/@santesson89?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="external nofollow noopener" target="_blank">Andrea De Santis</a> on <a href="https://unsplash.com/photos/black-and-white-robot-toy-on-red-wooden-table-zwd435-ewb4?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="external nofollow noopener" target="_blank">Unsplash</a> <br> <br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/tab_178_179_180.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/cart_800.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/actor_150_aspect.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Video animations of the agent's behavior in the `Stochastic Windy Gridworld` environment (left), the `Cartpole` environment (middle), and the `Catch` environment (right) at the end of training. Keep reading to learn more about the projects. </div> <h2 id="stochastic-windy-gridworld--tabular-reinforcement-learning">Stochastic Windy Gridworld – Tabular Reinforcement Learning</h2> <p>The <code class="language-plaintext highlighter-rouge">Stochastic Windy Gridworld</code> environment is an adaptation of one of the examples in the book <a class="citation" href="#reinforce_book">(Sutton &amp; Barto, 2018)</a>. You can see the environment in the figure below. The environment has a 10x7 grid, with a start point denoted by “S” and a goal state denoted by “G”. The agent can move in four directions: up, down, left, and right. The environment has a stochastic feature: the wind. The wind blows the agent up one or two additional steps (thin and thick arrows, respectively). The wind is present on 80% of the occasions, making the environment stochastic. The agent receives a reward of -1 at each step, a reward of +40 when reaching the goal state.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/tab_env.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/tab_env.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="tab_env" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The `Stochastic Windy Gridworld` environment. The start point is denoted with "S" and the goal state with "G". The arrows indicate the direction of the wind, thicker arrows indicate stronger wind. The agent can move in four directions: up, down, left, and right. </div> <p>The goal of this project was to study a range of basic principles in tabular, value-based reinforcement learning. The first part focused on Dynamic Programming, which is a bridging method between planning and reinforcement learning. In Dynamic Programming, we have full access to a model of the environment, and we can get the transition probabilities and rewards for any state and action. This guarantees that we will find the optimal solution, given enough iterations.</p> <p>We implemented the Dynamic Programming algorithm and applied it to the <code class="language-plaintext highlighter-rouge">Stochastic Windy Gridworld</code> environment. The figure below shows different iterations of the algorithm. The Q-values converge to the optimal policy, after 18 iterations. In the beginning, we see that the values are low in all states, as the agent has not yet learned the optimal policy. After 10 iterations, the agent has learned to navigate the environment and the Q-values are higher and more stable. After 18 iterations, the agent has learned the optimal policy and the Q-values guide the agent to the goal state, in the fewest number of steps, which is 23.3 steps on average. The fact that this number is not an integer is due to the stochastic nature of the environment.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/iteration_1.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/iteration_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="iteration_1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/iteration_10.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/iteration_10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="iteration_10" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/iteration_18.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/iteration_18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="iteration_18" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Different iterations of the Dynamic Programming algorithm applied to the `Stochastic Windy Gridworld` environment. </div> <p>The second part of the project focused on Model-free Reinforcement Learning. We implemented two agents:</p> <ul> <li>Q-learning</li> <li>SARSA The difference between these two agents is that Q-learning is an off-policy algorithm, while SARSA is an on-policy algorithm.</li> </ul> <p>For the exploration strategy, we implemented two methods:</p> <ul> <li>ε-greedy policy</li> <li>softmax (Boltzmann) policy This will help us explore the trade-off between exploration and exploitation by comparing the two methods and the hyperparameters that control the exploration in each method (ε for ε-greedy and temperature for softmax).</li> </ul> <p>Finally we also implemented the n-step back-ups and Monte Carlo back-ups for the Q-learning agent. The n-step back-ups are a generalization of the 1-step back-ups (used by the default Q-learning agent), where we sum more rewards in a trace. The Monte Carlo back-ups are a special case of n-step back-ups, where we sum all rewards until the end of the episode (no bootstrapping).</p> <p>For the first experiment, we compared the exploration strategies. The figure below shows the results of the experiment. On the left we have three different values of ε for the ε-greedy policy and three different values of the temperature for the softmax policy. We can see that the best performance comes from the two lowest values of the temperature (as good as the Dynamic Programming optimum). For this reason on the figure on the right we experimented with even lower values of ε which now perform almost as good as the softmax policy.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/exploration.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/exploration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="exploration" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/exploration_2.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/exploration_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="exploration 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Exploration experiment. Left: comparison of ε-greedy and softmax policies with different values of ε and temperature. Right: comparison of some lower values of ε. </div> <p>For the second experiment, we compared the on-policy (SARSA) and off-policy (Q-learning) algorithms. The figure below shows the results of the experiment. We can see that the Q-learning algorithm performs better than the SARSA algorithm. The off-policy algorithm, means that the agent learns the optimal policy, even if it is not the one that it is following. The on-policy algorithm, means that the agent learns the policy that the agent is following. For this environment, the off-policy training seems to be more effective.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/on_off_policy.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/on_off_policy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="on_off_policy" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/on_off_policy_2.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/on_off_policy_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="on_off_policy 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> On-policy vs. off-policy experiment. Left: comparison of Q-learning and SARSA algorithms for different values of the learning rate (α), using an ε-greedy policy with ε=0.1. Right: comparison of Q-learning and SARSA algorithms using a softmax policy with temperature=0.1. </div> <p>The final experiment compared the n-step back-ups and Monte Carlo back-ups for the Q-learning agent. Keep in mind that the 1-step back-ups are identical to the default Q-learning agent. We see the results of the experiment in the figure below. From the results, we can say that the default Q-learning agent (1-step back-ups) performs better than the n-step back-ups and Monte Carlo back-ups. The smaller the n, the better the performance. This is a classic trade-off between bias and variance. The higher the n, the lower the variance, but the higher the bias. For this particular environment, the lower bias of the 1-step back-ups seems to be more beneficial, despite the higher variance.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/depth.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/depth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="depth" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/depth_2.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/depth_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="depth 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Depth experiment. Left: comparison of Q-learning agents with 1-step, 3-step, 5-step, and Monte Carlo back-ups, using an ε-greedy policy with ε=0.1. Right: comparison of Q-learning agents with 1-step, 3-step, 5-step, and Monte Carlo back-ups, using a softmax policy with temperature=0.1. </div> <p>Finally, we created some cool animations of the agent’s behavior in the <code class="language-plaintext highlighter-rouge">Stochastic Windy Gridworld</code> environment. The three videos show the agent’s behavior at the beginning of training (episodes 3), in the middle of training (episodes 56 and 57), and at the end of training (episodes 179, 180, and 181). The agent uses the Q-learning algorithm with a softmax (Boltzmann) policy with a temperature of 0.01. At the beginning of training, the agent has not yet learned how to navigate the environment and is moving randomly, constantly exploring the world. We have sped up the video 2x because of the many steps the agent takes, to reach the goal state just once. In the middle of training, the agent has greatly improved its performance and is moving more efficiently towards the goal state. It manages to reach the goal state twice within the video. At the end of training, the agent has learned the optimal policy and is moving directly towards the goal state, reaching it in the fewest number of steps. The video is shorter than the other two and yet the agent reaches the goal state three times.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/tab_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/tab_55_56.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/tab_178_179_180.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Video animations of the agent's behavior in the `Stochastic Windy Gridworld` environment. From left to right: beginning of training (episodes 3), middle of training (episodes 56 and 57), and end of training (episodes 179, 180, and 181). The video for the beginning of training has been sped up 2x because of the many steps the agent takes. The agent learns to navigate the environment and reach the goal state while taking into consideration the wind that blows the agent up. The agent was trained using the Q-learning algorithm with a softmax (Boltzmann) policy with a temperature of 0.01. </div> <p>The assignment was a great introduction to reinforcement learning and the different algorithms used in the field. We solved the <code class="language-plaintext highlighter-rouge">Stochastic Windy Gridworld</code> environment using Dynamic Programming, Q-learning, and SARSA. We also explored various hyperparameters and strategies for exploration, on-policy vs. off-policy algorithms, and different back-ups. The animations of the agent’s behavior in the environment were a great way to visualize the agent’s learning process and how it improved over time. The full report can be found <a href="/assets/pdf/tabular.pdf">here</a>. The code for the project is not publicly available, as it is part of the course material. If requested, I can provide parts of the code privately.</p> <h2 id="cartpole--deep-q-learning">Cartpole – Deep Q-Learning</h2> <p>For the second project, we implemented a Deep Q-Learning agent to solve the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment from <a href="https://www.gymlibrary.dev/environments/classic_control/cart_pole/" rel="external nofollow noopener" target="_blank">OpenAI Gym</a> using PyTorch. The <code class="language-plaintext highlighter-rouge">Cartpole</code> environment is a classic control problem from the reinforcement learning literature <a class="citation" href="#control_problems">(Barto et al., 1983)</a>. The environment consists of a cart that can move left or right, and a pole that is attached to the cart. The goal is to balance the pole by applying forces to the cart. The agent receives a reward of +1 for every step taken, including the termination step. The episode ends if the pole angle is greater than ±12°, the cart position is greater than ±2.4 (agent reaches the edge of the display), or the episode length is greater than 500 (maximum number of steps). The action space is discrete with two actions: push the cart to the left or push the cart to the right. The observation space consists of four values: cart position, cart velocity, pole angle, and pole angular velocity.</p> <p>We implemented the Deep Q-Learning (DQN) algorithm with experience replay and target networks. The DQN agent uses a neural network to approximate the Q-values for each state-action pair. The pseudo-code for the DQN algorithm we implemented is from the book <a class="citation" href="#2022arXiv220102135P">(Plaat, 2022)</a>. We implemented DQN using PyTorch. We also implemented a Target Network (TN) to stabilize the learning process and an Experience Replay (ER) buffer to store and sample experiences for training. These features can be turned on or off by the user. We also implemented Double DQN (DDQN) to see if it improves the performance of the agent.</p> <p>We first use the DQN with both the TN and ER turned on to experiment with different exploration strategies. We compare the ε-greedy and Boltzmann exploration strategies, with different values of ε and temperature. We also experiment with linear annealing of ε and temperature. The results of the experiments are shown in the figures below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/egreedy_experiment.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/egreedy_experiment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="egreedy_experiment" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/egreedy_linear_anneal_experiment.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/egreedy_linear_anneal_experiment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="egreedy_anneal_experiment" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Exploration strategies experiment. Left: comparison of classic ε-greedy with different values of ε. Right: comparison of ε-greedy with linear annealing, with start value for ε of 0.99 and end value of 0.01. Different lines represent different values of the percentage of the total number of episodes at which the annealing ends. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/boltzmann_experiment.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/boltzmann_experiment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="boltzmann_experiment" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/boltzmann_linear_anneal_experiment.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/boltzmann_linear_anneal_experiment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="boltzmann_linear_anneal_experiment" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Exploration strategies experiment. Left: comparison of Boltzmann with different values of the temperature. Right: comparison of Boltzmann with linear annealing, with start value for the temperature of 2.00 and end value of 0.01. Different lines represent different values of the percentage of the total number of episodes at which the annealing ends. </div> <p>We also implemented a novelty-based exploration strategy, where the agent explores the environment based on the novelty of the state-action pairs. The original code for the novelty-based exploration strategy comes from <a class="citation" href="#2016arXiv161104717T">(Tang et al., 2016)</a>, and uses a hash function to calculate the novelty of the state-action pairs. We compare the best performing ε-greedy and Boltzmann strategies with and without linear annealing and the novelty-based exploration strategy. We also compare the performance of the DQN agent with the performance of the DDQN agent. The results of the experiments are shown in the figures below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/exploration_methods.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/exploration_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="exploration_methods" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/comparison_dqn_ddqn.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/comparison_dqn_ddqn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="comparison_dqn_ddqn" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: comparison of all exploration strategies withe the best exploration hyperparameter. Right: comparison of DQN and DDQN agents. </div> <p>After that we tune the hyperparameters of the DQN agent to find the best performing agent. For the optimazation we use the <code class="language-plaintext highlighter-rouge">Optuna</code> library. We can see some figures for the tuning of the most important hyperparameters below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/parallel_coordinate.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/parallel_coordinate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="parallel_coordinate" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/slice.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/slice.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="slice" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: parallel coordinates plot showing the values of the four most important hyperparameters for the DQN agent. Right: slice plot showing the values of the four most important hyperparameters for the DQN agent. </div> <p>The best exploration method was the Boltzmann exploration strategy without linear annealing for a temperature of 0.1. We used this value to perform the ablation study where we turned off either or both the TN and ER. The results of the ablation study are shown in the figure below. Note that in the legendd we denote the absence of the TN with “DQN-TN” and the absence of the ER with “DQN-ER”.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/ablation_study.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/ablation_study.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ablation_study" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ablation study of the Deep Q-Learning agent in the `Cartpole` environment. Left: comparison of DQN with both TN and ER (DQN), and DQN without both (DQN-ER-TN). Middle: comparison of DQN with both TN and ER (DQN), and DQN without TN (DQN-ER). Right: comparison of DQN with both TN and ER (DQN), and DQN without ER (DQN-TN). </div> <p>We see that although the TN might help with the stability of the learning process, the ER is crucial for the performance of the agent. The best performing agent was the DQN agent with both the TN and ER turned on.</p> <p>Finally, we train the best performing agent and save the weights of the neural network at various stages of training. We then evaluate the agent using the saved weights and create some cool video animations of the agent’s behavior in the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment. The videos show the agent’s behavior at the beginning of training (episodes 20), in the middle of training (episodes 200 and 400), and at the end of training (episodes 800). We see how the agent goes from completely poor performance at the beginning of training to a perfect performance at the end of training. At mid-training, the agent seems to struggle more staying within the limits of the environment, and seems to be doing better at balancing the pole. The videos are shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/cart_20.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/cart_200.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Video animations of the agent's behavior in the `Cartpole` environment, during different stages of the training process. Each video contains 10 evaluation episodes (no exploration or training). Left: beginning of training (after 20 episodes of training). Right: middle of training (after 200 episodes of training). </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/cart_400.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/cart_800.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Left: middle of training (after 400 episodes of training). Right: end of training (after 800 episodes of training). For this video we only included 5 evaluation episodes, as the agent was performing perfectly and we wanted to keep the video short. </div> <p>We can also create histograms of the rewards obtained by the agent during evaluation after 200, 400, and 800 episodes of training. The histograms are shown below. We can see that at the end of training, the agent is consistently obtaining the maximum reward of 500, which means that the agent is balancing the pole for the maximum number of steps (500 steps).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/hist_200.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/hist_200.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hist_200" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/hist_400.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/hist_400.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hist_400" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/hist_800.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/hist_800.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hist_800" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Histograms of the rewards obtained by the agent during evaluation after 200, 400, and 800 episodes of training. </div> <p>Overall, we were successful in training a Deep Q-Learning agent to solve the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment. We experimented with different exploration strategies, hyperparameters, and ablated the TN and ER. We found that the best performing agent was the DQN agent with both the TN and ER turned on, using the Boltzmann exploration strategy with a temperature of 0.1. We also implemented the DDQN agent and compared its performance with the DQN agent. We found that the DDQN agent performed at the same level as the DQN agent. The full report can be found <a href="/assets/pdf/cartpole.pdf">here</a>. The code for the project is not publicly available, as it is part of the course material. If requested, I can provide parts of the code privately.</p> <h2 id="catch--actor-critic">Catch – Actor-Critic</h2> <p>In this assignment we worked with a new environment called <code class="language-plaintext highlighter-rouge">Catch</code>, which is an extension from the <code class="language-plaintext highlighter-rouge">Catch</code> environment in <a class="citation" href="#2019arXiv190803568O">(Osband et al., 2019)</a>. The environment consists of a paddle that moves left or right to catch balls that drop from the top of the screen. It has a 7x7 grid, and the agent can move the paddle left, right, or stay idle. The agent receives a reward of +1 when catching a ball, a reward of -1 when missing a ball, and a reward of 0 otherwise. The episode ends when the agent reaches the maximum number of steps (default is 250 steps) or misses the maximum number of balls(default is 10 balls). The observation space can be either a vector with the xy-locations of the paddle and the lowest ball, or a binary two-channel pixel array with the paddle location in the first channel and all balls in the second channel. The speed of dropping new balls can be adjusted, as well as the size of the grid.</p> <p>We implemented three agents in this project:</p> <ul> <li>REINFORCE</li> <li>Actor-Critic</li> <li>Proximal Policy Optimization (PPO)</li> </ul> <p>All these agents use a policy-based algorithm, which is a different approach from the value-based algorithms (DQN and DDQN) used in the previous projects. The policy-based algorithms directly learn the policy, which is a mapping from states to actions, without learning the value function. The REINFORCE agent uses the Monte Carlo policy gradient method to update the policy. The Actor-Critic agent uses an actor network to learn the policy and a critic network to learn the value function. The PPO agent uses the Proximal Policy Optimization algorithm to update the policy, which utilizes a clipped objective function to prevent large policy updates.</p> <p>For the Actor-Critic agent, we also implemented a Baseline and Bootstrapping. The hyperparameters, we experimented with many different values to find the best performing agent. We performed an ablation study to compare the performance of the Actor-Critic agent with and without the Baseline and Bootstrapping, as well as the REINFORCE agent. The results of the ablation study are shown in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/ablation_ac.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/ablation_ac.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ablation_ac" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ablation study of the Actor-Critic agent in the `Catch` environment. </div> <p>As we can see from the results, the Actor-Critic agent greatly improves by using the Baseline and Bootstrapping. The vanilla Actor-Critic agent (no Baseline and Bootstrapping) is not able to learn at the environment. The REINFORCE agent performs better than the vanilla Actor-Critic agent, but not as good as the Actor-Critic agent with either the Baseline or Bootstrapping. The best performing agent is the Actor-Critic agent with both the Baseline and Bootstrapping turned on.</p> <p>Next we experiment with the vector and pixel (default) input for the <code class="language-plaintext highlighter-rouge">Catch</code> environment. We also compare the PPO agent for different values of the ε-clip parameter with the Actor-Critic agent. We experiment with different grid sizes and speeds for the Actor-Critic agent. We also use the Actor-Critic agent to solve the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment (see the previous project). All these experiments are shown in the figures below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/experiment_vector.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/experiment_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="experiment_vector" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/epsilon_clip_ppo.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/epsilon_clip_ppo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="epsilon_clip_ppo" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/cartpole_ac.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/cartpole_ac.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cartpole_ac" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: experiment with vector vs pixel input for the `Catch` environment. Middle: experiment with ε-clip for the PPO agent in the `Catch` environment. Right: experiment with the Actor-Critic agent in the `Cartpole` environment. </div> <p>As we can see the vector input makes the learning process more unstable, and does not reach the performance of the default pixel input. The PPO agent performs better with a lower value of the ε-clip parameter, but is slower to learn than the Actor-Critic agent. For the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment, the Actor-Critic agent shows some learning progress, but is not able to solve the environment. It should be noted that we did not spend much time tuning the hyperparameters for the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment, as the focus was on the <code class="language-plaintext highlighter-rouge">Catch</code> environment, so the results are still promising.</p> <p>After that we experimented with some environment variations. We used the best performing Actor-Critic agent to solve the environment with different grid sizes, speeds, and combinations of grid sizes and speeds. When the ball speed is greater than 1.0 (default), the new ball drops before the previous ball reaches the bottom row. The opposite is true when the ball speed is less than 1.0. The results of the experiments are shown in the figures below. To compare the performance of the agent, we used the fraction of caught balls for each episode.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/grid_size_ac.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/grid_size_ac.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="grid_size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/speed_ac.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/speed_ac.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="speed" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/speed_size_ac.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/speed_size_ac.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="speed_size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: experiment with different grid sizes for the Actor-Critic agent in the `Catch` environment. Middle: experiment with different speeds for the Actor-Critic agent in the `Catch` environment. Right: experiment with different grid sizes and speeds for the Actor-Critic agent in the `Catch` environment. For all the figures, the y-axis shows the fraction of caught balls for each episode (x-axis). </div> <p>As we can see, making the grid smaller helped the agent to learn faster, but increasing it too much made the learning process unstable and suboptimal. Increasing the speed of the balls also lowered the performance of the agent, it was however expected, since it was no longer possible to catch all the balls, even when playing perfectly. What is surprising is that a slower speed for the balls made the learning process very unstable, with some runs reaching the optimal performance, and others being very far from it.</p> <p>Finally, we created some nice video animations of the performance of our best agent (Actor-Critic with Baseline and Bootstrapping) at various stages of the learning process. The videos show the agent’s behavior at the start of training (after 0 epochs), in the first stages of training (after 50 epochs), in the middle of training (after 100 epochs), and at the end of training (after 150 epochs). The agent kept learning after that, but the performance did not improve as it had already reached the optimal performance. For each evaluation we turned off the exploration and training, and the agent played 10 episodes. The videos only show 5 episodes for the first two stages and 3 episodes for the last two stages, to keep the videos short. The videos are shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/actor_0.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/actor_50.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Video animations of the agent's behavior in the `Catch` environment, during different stages of the training process. Each video contains 5 evaluation episodes (no exploration or training). Left: beginning of training (after 0 epochs of training). Right: first stages of training (after 50 epochs of training). </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/actor_100.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/actor_150.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Video animations of the agent's behavior in the `Catch` environment, during different stages of the training process. Each video contains 3 evaluation episodes (no exploration or training). Left: middle of training (after 100 epochs of training). Right: end of training (after 150 epochs of training). </div> <p>We can see that at 0 epochs of training, the agent is moving randomly and is only able to catch a few balls, just by chance. After 50 epochs of training, the agent still moves more or less randomly, but is able to catch some more balls. After 100 epochs of training, the agent has learned to move more efficiently and is able to catch most of the balls. At 150 epochs of training, the agent has learned the optimal policy and is able to catch all the balls. We can also see that in the histograms of the rewards obtained by the agent during evaluation in all four stages of training for the 10 episodes. The histograms are shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/actor_0.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/actor_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="actor_0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/actor_50.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/actor_50.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="actor_50" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/actor_100.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/actor_100.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="actor_100" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/courses/reinforce/actor_150.png" sizes="95vw"></source> <img src="/assets/img/courses/reinforce/actor_150.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="actor_150" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Histograms of the rewards obtained by the agent during evaluation after 0, 50, 100, and 150 epochs of training (from left to right). </div> <p>In conclusion, we were able to train an Actor-Critic agent to solve the <code class="language-plaintext highlighter-rouge">Catch</code> environment. We experimented with different hyperparameters, ablated the Baseline and Bootstrapping, and compared the performance of the Actor-Critic agent with the REINFORCE agent. We also experimented with the vector and pixel input for the <code class="language-plaintext highlighter-rouge">Catch</code> environment, and compared the PPO agent with the Actor-Critic agent. We experimented with different grid sizes and speeds for the <code class="language-plaintext highlighter-rouge">Catch</code> environment, and used the Actor-Critic agent to try to solve the <code class="language-plaintext highlighter-rouge">Cartpole</code> environment. The full report can be found <a href="/assets/pdf/catch.pdf">here</a>. The code for the project is not publicly available, as it is part of the course material. If requested, I can provide parts of the code privately.</p> <h2 id="references">References</h2> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="2022arXiv220102135P" class="col-sm-8"> <div class="title">Deep Reinforcement Learning, a textbook</div> <div class="author"> Aske Plaat </div> <div class="periodical"> <em>arXiv e-prints</em>, Jan 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="2019arXiv190803568O" class="col-sm-8"> <div class="title">Behaviour Suite for Reinforcement Learning</div> <div class="author"> Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, Hado Van Hasselt' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>arXiv e-prints</em>, Aug 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="reinforce_book" class="col-sm-8"> <div class="title">Reinforcement Learning: An Introduction</div> <div class="author"> Richard S. Sutton, and Andrew G. Barto </div> <div class="periodical"> Aug 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="2016arXiv161104717T" class="col-sm-8"> <div class="title">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</div> <div class="author"> Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>arXiv e-prints</em>, Nov 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">1983</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="control_problems" class="col-sm-8"> <div class="title">Neuronlike adaptive elements that can solve difficult learning control problems</div> <div class="author"> Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson </div> <div class="periodical"> <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, Nov 1983 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ioannis Koutalios. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: January 02, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Publications in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"Some of the projects I have worked on. The projects are categorized into research, course and personal projects. You can find more information about each project by clicking on the cards below.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-advent-of-code",title:"Advent of Code",description:"programming puzzles from the famous Advent of Code challenge",section:"Projects",handler:()=>{window.location.href="/projects/AdventOfCode/"}},{id:"projects-data-mining",title:"Data Mining",description:"projects from the course 'Data Mining' at Leiden University",section:"Projects",handler:()=>{window.location.href="/projects/DataMining/"}},{id:"projects-deep-learning",title:"Deep Learning",description:"projects from the course 'Deep Learning' at Leiden University",section:"Projects",handler:()=>{window.location.href="/projects/DeepLearning/"}},{id:"projects-applying-a-cross-power-model-for-power-suppression-in-flamingo",title:"Applying a cross-power model for power suppression in FLAMINGO",description:"Master's thesis research project (2nd year)",section:"Projects",handler:()=>{window.location.href="/projects/FLAMINGO/"}},{id:"projects-gravitational-wave-hackathon",title:"Gravitational Wave Hackathon",description:"g2net hackathon in Malta (2020) and Thessaloniki (2023)",section:"Projects",handler:()=>{window.location.href="/projects/GW_hackathon/"}},{id:"projects-model-free-reinforcement-learning-for-sensorless-adaptive-optics",title:"Model-Free Reinforcement Learning for Sensorless Adaptive Optics",description:"research project on Advanced Reinforcement Learning",section:"Projects",handler:()=>{window.location.href="/projects/RLAdaptive/"}},{id:"projects-other-astronomical-projects",title:"Other Astronomical Projects",description:"projects from various astronomical, astrophysical, and mathematical courses",section:"Projects",handler:()=>{window.location.href="/projects/astronomical/"}},{id:"projects-finding-the-best-exoplanets-to-search-for-exomoons-by-radial-velocity",title:"Finding the best exoplanets to search for exomoons by radial velocity",description:"Master's thesis research project (1st year)",section:"Projects",handler:()=>{window.location.href="/projects/exomoons/"}},{id:"projects-kaggle-challenges",title:"Kaggle Challenges",description:"collection of Kaggle challenges I have participated in",section:"Projects",handler:()=>{window.location.href="/projects/kaggle/"}},{id:"projects-numerical-analysis",title:"Numerical Analysis",description:"projects in numerical analysis",section:"Projects",handler:()=>{window.location.href="/projects/numerical/"}},{id:"projects-reinforcement-learning",title:"Reinforcement Learning",description:"projects from the course 'Reinforcement Learning' at Leiden University",section:"Projects",handler:()=>{window.location.href="/projects/reinforcement/"}},{id:"projects-spectral-classification-of-neutron-star-post-merger-gravitational-wave-emission",title:"Spectral Classification of Neutron Star Post-Merger Gravitational Wave Emission",description:"Bachelor's thesis research project",section:"Projects",handler:()=>{window.location.href="/projects/spectral/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%6B%6F%75%74%61%6C%69%6F%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/306989855203","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0004-5933-5371","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/johnkou97","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/koutalios-ioannis","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/ioanniskoutalios","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>